---
title: PCA學習與應用
date: 2018-01-14 15:46:35
comments: true
categories: data science
---
PCA 是一種特徵提取的技術，利用特徵降維來萃取出資料中最有代表性的主成分。而要注意的是 PCA 並不是從原本資料特徵中捨棄不重要的特徵來降維，而是由這些特徵與特徵向量 (eigenvector) 的線性組合所產生的新特徵來代表原本的資料。

![](https://i.imgur.com/Ddhov6x.png)


X 表示一個 **n x d** 的資料集，n 是紀錄數，d 是資料維度也是特徵數，而 W 就是由 X 的共變異數矩陣所計算出來的特徵向量矩陣，**d x k** 維，其中 1 <= k <= d ，k 就是指定的主成分數，即表示要留下來的特徵向量個數， **X*W 就是 PCA 計算結果**，矩陣的維度變為 **(n x d)*(d x k)=n x k** ，如此達到降維的效果。

![](https://i.imgur.com/hZH712X.png)

### PCA 演算法步驟
1. 標準化 d 維原資料集
2. 建立共變異數矩陣(covariance matrix)
3. 分解共變異數矩陣(covariance matrix)為特徵向量(eigenvector)與特徵值(eigenvalues)。
4. 選取 k 個最大特徵值(eigenvalues)相對應k個的特徵向量(eigenvector)，其中 k 即為新特徵子空間的維數。
5. 使用排序最上面的 k 個的特徵向量(eigenvector)，建立投影矩陣(project matrix)W。
6. 使用投影矩陣(project matrix) W 轉換原本 d 維的原數據至新的 k 維特徵子空間。

### PCA 異常偵測
PCA 本質上可以想成一個將資料去除雜質的方法，因此可以藉由比較資料去除雜質的差異，來找出這些雜質 (異常)。首先，可以簡單想成 PCA 是由一個原本矩陣 X 乘上投影矩陣 W 得到的結果。因此當然也就可以從 PCA 的結果乘上 W 的轉置矩陣來還原到原本的矩陣空間。因此得到以下公式:

$$\underset{n\times k}{Z} = \underset{n\times d}{X} \times\underset{d\times k}{W}$$

$$\underset{n\times d}{\bar{X}} = \underset{n\times k}{Z}\times\underset{k\times d}{W^\top}$$

$\underset{n\times d}{\bar{X}}$ 就是從 PCA 空間還原的結果，而計算異常的方式就是計算 $\underset{n\times d}{\bar{X}}$ 與 $\underset{n\times d}{X}$ 兩者的歐式距離，即所謂的 **reconstruction error** ，如下圖所示新的測試資料點與經過 PCA 還原的點的距離就是該點的 reconstruction error，距離越大表示越有可能為異常。

![](https://i.stack.imgur.com/1j5X1.png=100x100)

### PCA 異常偵測缺點
1. 不適合在資料量大的情況，拿全部的訓練資料來進行矩陣計算，資料量大時極耗運算資源。
2. reconstruction error 必須要自行定義 threshold 值來判斷正常或是異常，無法自動得知。



參考資料:
https://stats.stackexchange.com/questions/259806/anomaly-detection-using-pca-reconstruction-error
https://www.kaggle.com/ericlikedata/reconstruct-error-of-pca/data
https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com
https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
http://arbu00.blogspot.tw/2017/02/6-principal-component-analysispca.html